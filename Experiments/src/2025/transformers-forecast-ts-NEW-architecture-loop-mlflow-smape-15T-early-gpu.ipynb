{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b8d320",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mlflow joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afaafbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/04 17:03:58 INFO mlflow.tracking.fluent: Experiment with name 'Prometheus_Transformer_Experiment_MRFO_RMSE_EarlyStopping_GPU' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "\n",
      "Iteration 1/10\n",
      "  Manta 1/20: Fitness (RMSE) = 17.3528\n",
      "  New Best Fitness: 17.3528\n",
      "  Manta 2/20: Fitness (RMSE) = 31.0312\n",
      "  Manta 3/20: Fitness (RMSE) = 23.3058\n",
      "  Manta 4/20: Fitness (RMSE) = 18.0656\n",
      "  Manta 5/20: Fitness (RMSE) = 30.5157\n",
      "  Manta 6/20: Fitness (RMSE) = 23.4607\n",
      "  Manta 7/20: Fitness (RMSE) = 29.9300\n",
      "  Manta 8/20: Fitness (RMSE) = 25.5893\n",
      "  Manta 9/20: Fitness (RMSE) = 20.8805\n",
      "  Manta 10/20: Fitness (RMSE) = 16.8775\n",
      "  New Best Fitness: 16.8775\n",
      "  Manta 11/20: Fitness (RMSE) = 24.1162\n",
      "  Manta 12/20: Fitness (RMSE) = 18.1410\n",
      "  Manta 13/20: Fitness (RMSE) = 20.1752\n",
      "  Manta 14/20: Fitness (RMSE) = 17.4709\n",
      "  Manta 15/20: Fitness (RMSE) = 18.1613\n",
      "  Manta 16/20: Fitness (RMSE) = 21.7262\n",
      "  Manta 17/20: Fitness (RMSE) = 19.9124\n",
      "  Manta 18/20: Fitness (RMSE) = 24.8061\n",
      "  Manta 19/20: Fitness (RMSE) = 13.8076\n",
      "  New Best Fitness: 13.8076\n",
      "  Manta 20/20: Fitness (RMSE) = 19.2149\n",
      "üèÉ View run MRFO_Iteration_1 at: http://localhost:5001/#/experiments/8/runs/504d4176c9964e66a9969d58369e0349\n",
      "üß™ View experiment at: http://localhost:5001/#/experiments/8\n",
      "\n",
      "Iteration 2/10\n",
      "  Manta 1/20: Fitness (RMSE) = 19.5306\n",
      "  Manta 2/20: Fitness (RMSE) = 19.3083\n",
      "  Manta 3/20: Fitness (RMSE) = 17.1980\n",
      "  Manta 4/20: Fitness (RMSE) = 30.0251\n",
      "  Manta 5/20: Fitness (RMSE) = 14.4630\n",
      "  Manta 6/20: Fitness (RMSE) = 21.9728\n",
      "  Manta 7/20: Fitness (RMSE) = 16.7840\n",
      "  Manta 8/20: Fitness (RMSE) = 15.9893\n",
      "  Manta 9/20: Fitness (RMSE) = 23.9953\n",
      "  Manta 10/20: Fitness (RMSE) = 21.4879\n",
      "  Manta 11/20: Fitness (RMSE) = 20.6781\n",
      "  Manta 12/20: Fitness (RMSE) = 20.2871\n",
      "  Manta 13/20: Fitness (RMSE) = 17.0053\n",
      "  Manta 14/20: Fitness (RMSE) = 19.1659\n",
      "  Manta 15/20: Fitness (RMSE) = 17.1637\n",
      "  Manta 16/20: Fitness (RMSE) = 19.1411\n",
      "  Manta 17/20: Fitness (RMSE) = 22.3776\n",
      "  Manta 18/20: Fitness (RMSE) = 23.5777\n",
      "  Manta 19/20: Fitness (RMSE) = 15.1906\n",
      "  Manta 20/20: Fitness (RMSE) = 15.5531\n",
      "üèÉ View run MRFO_Iteration_2 at: http://localhost:5001/#/experiments/8/runs/9b4a0cb5baaf4ccf9cc3d537f0e7ca44\n",
      "üß™ View experiment at: http://localhost:5001/#/experiments/8\n",
      "\n",
      "Early stopping triggered after 2 iterations due to no improvement for 5 iterations.\n",
      "\n",
      "Best MRFO Configuration: LR=0.0007352993570001823, Layers=3, Heads=8, FF=256\n",
      "  Repetition 1/5\n",
      "    MAE: 7.373950004577637, RMSE: 13.745038056222041, MAPE: 0.931306928396225%, SMAPE: 0.9438521862030029%\n",
      "  Repetition 2/5\n",
      "    MAE: 8.29460620880127, RMSE: 14.393526296330725, MAPE: 1.0392861440777779%, SMAPE: 1.0513622760772705%\n",
      "  Repetition 3/5\n",
      "    MAE: 6.27353572845459, RMSE: 12.58597023389598, MAPE: 0.7790566887706518%, SMAPE: 0.7901086211204529%\n",
      "  Repetition 4/5\n",
      "    MAE: 7.701728820800781, RMSE: 12.907467098899756, MAPE: 0.9523694403469563%, SMAPE: 0.9629086852073669%\n",
      "  Repetition 5/5\n",
      "    MAE: 9.949475288391113, RMSE: 14.649238519765605, MAPE: 1.2391556054353714%, SMAPE: 1.2482640743255615%\n",
      "  Average MAE: 7.918659210205078 (¬±1.2096072314533854), Average RMSE: 13.656248041022824 (¬±0.8054390687243465)\n",
      "  Average MAPE: 0.9882349614053965% (¬±0.15089736852457603), Average SMAPE: 0.9992992281913757% (¬±0.15023541450500488)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/05/04 17:21:33 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run Best_MRFO_Run at: http://localhost:5001/#/experiments/8/runs/618286b9adb447c795bd3b1703727c97\n",
      "üß™ View experiment at: http://localhost:5001/#/experiments/8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "# Configurar MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5001\")\n",
    "mlflow.set_experiment(\"Prometheus_Transformer_Experiment_MRFO_RMSE_EarlyStopping_GPU\")\n",
    "\n",
    "# Definir o dispositivo (usar MPS para Apple Silicon GPU se dispon√≠vel)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Definindo constantes\n",
    "DATA_DIR = \"../../data/\"\n",
    "FILE_PATH = os.path.join(DATA_DIR, 'ts.pkl')\n",
    "SEQ_LENGTH = 48  # 12 horas (48 * 15min)\n",
    "MB = 1_048_576\n",
    "\n",
    "# 1. Carregar e reamostrar os dados para 15 minutos\n",
    "df = pd.read_pickle(FILE_PATH)\n",
    "ts = df['value'].astype(float).resample('15min').mean().dropna()\n",
    "dates = ts.index\n",
    "\n",
    "# 2. Dividir os dados: 60% treino, 20% valida√ß√£o, 20% teste\n",
    "train_size = int(0.6 * len(ts))\n",
    "val_size = int(0.2 * len(ts))\n",
    "train = ts[:train_size]\n",
    "val = ts[train_size:train_size + val_size]\n",
    "test = ts[train_size + val_size:]\n",
    "\n",
    "# 3. Escalonar os dados\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train.values.reshape(-1, 1))\n",
    "val_scaled = scaler.transform(val.values.reshape(-1, 1))\n",
    "test_scaled = scaler.transform(test.values.reshape(-1, 1))\n",
    "\n",
    "# 4. Criar sequ√™ncias\n",
    "def create_sequences(data, dates, seq_length):\n",
    "    X, y, y_dates = [], [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i + seq_length])\n",
    "        y.append(data[i + seq_length])\n",
    "        y_dates.append(dates[i + seq_length])\n",
    "    return np.array(X), np.array(y), np.array(y_dates)\n",
    "\n",
    "X_train, y_train, y_dates_train = create_sequences(train_scaled, dates[:train_size], SEQ_LENGTH)\n",
    "X_val, y_val, y_dates_val = create_sequences(val_scaled, dates[train_size:train_size + val_size], SEQ_LENGTH)\n",
    "X_test, y_test, y_dates_test = create_sequences(test_scaled, dates[train_size + val_size:], SEQ_LENGTH)\n",
    "\n",
    "# 5. Ajustar dimens√µes para o modelo Transformer\n",
    "d_model = 128\n",
    "X_train = np.repeat(X_train, d_model, axis=2)\n",
    "X_val = np.repeat(X_val, d_model, axis=2)\n",
    "X_test = np.repeat(X_test, d_model, axis=2)\n",
    "\n",
    "# 6. Converter para tensores PyTorch e mover para o dispositivo\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# 7. Definir codifica√ß√£o posicional\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "# 8. Definir o modelo Transformer (sem dropout adicional)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_layers, dim_feedforward):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.linear_out = nn.Linear(d_model, 1, bias=True)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = self.linear_out(output[:, -1, :])\n",
    "        return output\n",
    "\n",
    "# Outros hiperpar√¢metros fixos\n",
    "input_dim = d_model\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "\n",
    "# Fun√ß√£o para calcular SMAPE\n",
    "def smape(y_true, y_pred):\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    return 100 * np.mean(diff)\n",
    "\n",
    "# Fun√ß√£o de treinamento e avalia√ß√£o para uma repeti√ß√£o\n",
    "def train_and_evaluate(learning_rate, num_layers, nhead, dim_feedforward):\n",
    "    model = Encoder(input_dim, d_model, nhead, num_layers, dim_feedforward).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            X_batch = X_train[i:i + batch_size]\n",
    "            y_batch = y_train[i:i + batch_size]\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_val_pred = model(X_val)\n",
    "            val_loss = criterion(y_val_pred, y_val)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test)\n",
    "    \n",
    "    # Mover os dados de volta para a CPU para c√°lculos de m√©tricas\n",
    "    y_pred_rescaled = scaler.inverse_transform(y_pred.cpu().numpy()) / MB\n",
    "    y_test_rescaled = scaler.inverse_transform(y_test.cpu().numpy()) / MB\n",
    "    \n",
    "    mae = mean_absolute_error(y_test_rescaled, y_pred_rescaled)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_rescaled, y_pred_rescaled))\n",
    "    mape = mean_absolute_percentage_error(y_test_rescaled, y_pred_rescaled) * 100\n",
    "    smape_val = smape(y_test_rescaled, y_pred_rescaled)\n",
    "    \n",
    "    return mae, rmse, mape, smape_val, model\n",
    "\n",
    "# Fun√ß√£o de mapeamento de valores cont√≠nuos para discretos\n",
    "def map_continuous_to_discrete(value, discrete_values):\n",
    "    idx = int(round(value * (len(discrete_values) - 1)))\n",
    "    idx = max(0, min(idx, len(discrete_values) - 1))\n",
    "    return discrete_values[idx]\n",
    "\n",
    "# Fun√ß√£o de avalia√ß√£o para uma manta (usada no MRFO)\n",
    "def evaluate_manta(params, n_repetitions):\n",
    "    lr = 10 ** params[0]  # learning_rate (log scale)\n",
    "    nl = map_continuous_to_discrete(params[1], [2, 3])  # num_layers\n",
    "    nh = map_continuous_to_discrete(params[2], [4, 8])  # nhead\n",
    "    df = map_continuous_to_discrete(params[3], [256, 512])  # dim_feedforward\n",
    "\n",
    "    # Executar repeti√ß√µes sequencialmente (GPU n√£o se beneficia de paralelismo aqui)\n",
    "    results = [train_and_evaluate(lr, nl, nh, df) for _ in range(n_repetitions)]\n",
    "    rmse_list = [result[1] for result in results]  # Minimizar RMSE\n",
    "    return np.mean(rmse_list)\n",
    "\n",
    "# Implementa√ß√£o do MRFO com sa√≠das intermedi√°rias e early stopping\n",
    "class MRFO:\n",
    "    def __init__(self, objective_func, bounds, n_mantas=30, max_iter=100, patience=5, n_repetitions=5):\n",
    "        self.objective_func = objective_func\n",
    "        self.bounds = np.array(bounds).T  # Shape: (2, dim)\n",
    "        self.n_mantas = n_mantas\n",
    "        self.max_iter = max_iter\n",
    "        self.patience = patience\n",
    "        self.n_repetitions = n_repetitions\n",
    "        self.dim = self.bounds.shape[1]\n",
    "        \n",
    "        # Inicializar popula√ß√£o\n",
    "        self.positions = np.zeros((self.n_mantas, self.dim))\n",
    "        for d in range(self.dim):\n",
    "            self.positions[:, d] = np.random.uniform(self.bounds[0, d], self.bounds[1, d], self.n_mantas)\n",
    "        self.fitness = np.array([float('inf')] * self.n_mantas)\n",
    "        self.best_position = None\n",
    "        self.best_fitness = float('inf')\n",
    "        self.no_improvement_count = 0  # Contador para early stopping\n",
    "\n",
    "    def optimize(self):\n",
    "        for t in range(self.max_iter):\n",
    "            print(f\"\\nIteration {t+1}/{self.max_iter}\")\n",
    "            # Avaliar fitness de todas as mantas sequencialmente (GPU n√£o se beneficia de paralelismo aqui)\n",
    "            fitness_results = [self.objective_func(self.positions[i], self.n_repetitions) for i in range(self.n_mantas)]\n",
    "            self.fitness = np.array(fitness_results)\n",
    "\n",
    "            # Exibir resultados e atualizar o melhor fitness\n",
    "            for i in range(self.n_mantas):\n",
    "                print(f\"  Manta {i+1}/{self.n_mantas}: Fitness (RMSE) = {self.fitness[i]:.4f}\")\n",
    "                if self.fitness[i] < self.best_fitness:\n",
    "                    self.best_fitness = self.fitness[i]\n",
    "                    self.best_position = self.positions[i].copy()\n",
    "                    self.no_improvement_count = 0  # Resetar o contador\n",
    "                    print(f\"  New Best Fitness: {self.best_fitness:.4f}\")\n",
    "                else:\n",
    "                    self.no_improvement_count += 1\n",
    "\n",
    "            # Registrar melhor fitness no MLflow\n",
    "            with mlflow.start_run(run_name=f\"MRFO_Iteration_{t+1}\"):\n",
    "                mlflow.log_metric(\"best_fitness_rmse\", self.best_fitness)\n",
    "                # Registrar os hiperpar√¢metros correspondentes ao melhor fitness\n",
    "                lr = 10 ** self.best_position[0]\n",
    "                nl = map_continuous_to_discrete(self.best_position[1], [2, 3])\n",
    "                nh = map_continuous_to_discrete(self.best_position[2], [4, 8])\n",
    "                df = map_continuous_to_discrete(self.best_position[3], [256, 512])\n",
    "                mlflow.log_param(\"learning_rate\", lr)\n",
    "                mlflow.log_param(\"num_layers\", nl)\n",
    "                mlflow.log_param(\"nhead\", nh)\n",
    "                mlflow.log_param(\"dim_feedforward\", df)\n",
    "\n",
    "            # Crit√©rio de parada precoce\n",
    "            if self.no_improvement_count >= self.patience:\n",
    "                print(f\"\\nEarly stopping triggered after {t+1} iterations due to no improvement for {self.patience} iterations.\")\n",
    "                break\n",
    "\n",
    "            # Atualizar posi√ß√µes usando Chain Foraging, Cyclone Foraging e Somersault Foraging\n",
    "            for i in range(self.n_mantas):\n",
    "                r = np.random.random(self.dim)\n",
    "                r1 = np.random.random()\n",
    "\n",
    "                # Chain Foraging\n",
    "                if r1 < 0.5:\n",
    "                    if i == 0:\n",
    "                        self.positions[i] = self.positions[i] + r * (self.best_position - self.positions[i]) + \\\n",
    "                                            r * (self.best_position - self.positions[i])\n",
    "                    else:\n",
    "                        self.positions[i] = self.positions[i] + r * (self.positions[i-1] - self.positions[i]) + \\\n",
    "                                            r * (self.best_position - self.positions[i])\n",
    "\n",
    "                # Cyclone Foraging\n",
    "                else:\n",
    "                    beta = 2 * np.exp(r1 * (self.max_iter - t + 1) / self.max_iter) * np.sin(2 * np.pi * r1)\n",
    "                    if r1 < 0.5:\n",
    "                        self.positions[i] = self.positions[i] + r * (self.best_position - beta * self.positions[i])\n",
    "                    else:\n",
    "                        idx = np.random.randint(0, self.n_mantas)\n",
    "                        self.positions[i] = self.positions[i] + r * (self.positions[idx] - beta * self.positions[i])\n",
    "\n",
    "                # Somersault Foraging\n",
    "                r2 = np.random.random()\n",
    "                self.positions[i] = self.positions[i] + 0.5 * (self.best_position + self.positions[i]) * (2 * r2 - 1)\n",
    "\n",
    "                # Garantir que as posi√ß√µes estejam dentro dos limites\n",
    "                self.positions[i] = np.clip(self.positions[i], self.bounds[0], self.bounds[1])\n",
    "\n",
    "        return self.best_position, self.best_fitness\n",
    "\n",
    "# MRFO para otimizar hiperpar√¢metros\n",
    "n_repetitions = 5\n",
    "bounds = [\n",
    "    [-3.3, -2.7],  # log10(learning_rate): [0.0005, 0.002]\n",
    "    [0, 1],        # num_layers (mapeado para [2, 3])\n",
    "    [0, 1],        # nhead (mapeado para [4, 8])\n",
    "    [0, 1],        # dim_feedforward (mapeado para [256, 512])\n",
    "]\n",
    "\n",
    "mrfo = MRFO(lambda params, reps: evaluate_manta(params, reps), bounds, n_mantas=20, max_iter=10, patience=5, n_repetitions=n_repetitions)\n",
    "best_position, best_fitness = mrfo.optimize()\n",
    "\n",
    "# Mapear a melhor posi√ß√£o para hiperpar√¢metros\n",
    "best_lr = 10 ** best_position[0]\n",
    "best_nl = map_continuous_to_discrete(best_position[1], [2, 3])\n",
    "best_nh = map_continuous_to_discrete(best_position[2], [4, 8])\n",
    "best_df = map_continuous_to_discrete(best_position[3], [256, 512])\n",
    "\n",
    "# Treinar o modelo com a melhor configura√ß√£o para obter m√©tricas finais\n",
    "with mlflow.start_run(run_name=\"Best_MRFO_Run\"):\n",
    "    # Registrar hiperpar√¢metros\n",
    "    mlflow.log_param(\"learning_rate\", best_lr)\n",
    "    mlflow.log_param(\"num_layers\", best_nl)\n",
    "    mlflow.log_param(\"nhead\", best_nh)\n",
    "    mlflow.log_param(\"dim_feedforward\", best_df)\n",
    "    mlflow.log_param(\"seq_length\", SEQ_LENGTH)\n",
    "    mlflow.log_param(\"resample_interval\", \"15min\")\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
    "\n",
    "    print(f\"\\nBest MRFO Configuration: LR={best_lr}, Layers={best_nl}, Heads={best_nh}, FF={best_df}\")\n",
    "    \n",
    "    # Executar repeti√ß√µes sequencialmente (GPU n√£o se beneficia de paralelismo aqui)\n",
    "    results = [train_and_evaluate(best_lr, best_nl, best_nh, best_df) for _ in range(n_repetitions)]\n",
    "\n",
    "    mae_list = [result[0] for result in results]\n",
    "    rmse_list = [result[1] for result in results]\n",
    "    mape_list = [result[2] for result in results]\n",
    "    smape_list = [result[3] for result in results]\n",
    "    models = [result[4] for result in results]\n",
    "\n",
    "    for rep, (mae, rmse, mape, smape_val, _) in enumerate(results):\n",
    "        print(f\"  Repetition {rep+1}/{n_repetitions}\")\n",
    "        print(f\"    MAE: {mae}, RMSE: {rmse}, MAPE: {mape}%, SMAPE: {smape_val}%\")\n",
    "\n",
    "    avg_mae = np.mean(mae_list)\n",
    "    avg_rmse = np.mean(rmse_list)\n",
    "    avg_mape = np.mean(mape_list)\n",
    "    avg_smape = np.mean(smape_list)\n",
    "    std_mae = np.std(mae_list)\n",
    "    std_rmse = np.std(rmse_list)\n",
    "    std_mape = np.std(mape_list)\n",
    "    std_smape = np.std(smape_list)\n",
    "\n",
    "    print(f\"  Average MAE: {avg_mae} (¬±{std_mae}), Average RMSE: {avg_rmse} (¬±{std_rmse})\")\n",
    "    print(f\"  Average MAPE: {avg_mape}% (¬±{std_mape}), Average SMAPE: {avg_smape}% (¬±{std_smape})\")\n",
    "\n",
    "    # Registrar m√©tricas no MLflow\n",
    "    mlflow.log_metric(\"avg_mae\", avg_mae)\n",
    "    mlflow.log_metric(\"std_mae\", std_mae)\n",
    "    mlflow.log_metric(\"avg_rmse\", avg_rmse)\n",
    "    mlflow.log_metric(\"std_rmse\", std_rmse)\n",
    "    mlflow.log_metric(\"avg_mape\", avg_mape)\n",
    "    mlflow.log_metric(\"std_mape\", std_mape)\n",
    "    mlflow.log_metric(\"avg_smape\", avg_smape)\n",
    "    mlflow.log_metric(\"std_smape\", std_smape)\n",
    "\n",
    "    best_model = models[0]\n",
    "    mlflow.pytorch.log_model(best_model, \"best_model\")\n",
    "\n",
    "# 9. Fazer previs√µes\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_train_pred = best_model(X_train)\n",
    "    y_test_pred = best_model(X_test)\n",
    "\n",
    "# 10. Reverter o escalonamento e converter para MB\n",
    "y_train_pred_mb = scaler.inverse_transform(y_train_pred.cpu().numpy()) / MB\n",
    "y_train_mb = scaler.inverse_transform(y_train.cpu().numpy()) / MB\n",
    "y_test_pred_mb = scaler.inverse_transform(y_test_pred.cpu().numpy()) / MB\n",
    "y_test_mb = scaler.inverse_transform(y_test.cpu().numpy()) / MB\n",
    "\n",
    "# 11. Preparar dados para plotagem\n",
    "train_df = pd.DataFrame({\n",
    "    'date': y_dates_train,\n",
    "    'actual': y_train_mb.flatten(),\n",
    "    'predicted': y_train_pred_mb.flatten()\n",
    "}).sort_values('date')\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'date': y_dates_test,\n",
    "    'actual': y_test_mb.flatten(),\n",
    "    'predicted': y_test_pred_mb.flatten()\n",
    "}).sort_values('date')\n",
    "\n",
    "# 12. Plotar os resultados\n",
    "plt.style.use('default')\n",
    "fig, axs = plt.subplots(2, 1, figsize=(15, 10), sharex=False)\n",
    "\n",
    "axs[0].plot(train_df['date'], train_df['actual'], label='Real', color='blue', linewidth=1.5)\n",
    "axs[0].plot(train_df['date'], train_df['predicted'], label='Predito', color='red', alpha=0.7, linewidth=1.5)\n",
    "axs[0].set_title('Conjunto de Treinamento (60%)', fontsize=12, pad=10)\n",
    "axs[0].set_ylabel('Consumo de Mem√≥ria (MB)', fontsize=10)\n",
    "axs[0].legend(loc='upper left', fontsize=10)\n",
    "axs[0].grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "axs[1].plot(test_df['date'], test_df['actual'], label='Real', color='blue', linewidth=1.5)\n",
    "axs[1].plot(test_df['date'], test_df['predicted'], label='Predito', color='red', alpha=0.7, linewidth=1.5)\n",
    "axs[1].set_title('Conjunto de Teste (20%)', fontsize=12, pad=10)\n",
    "axs[1].set_xlabel('Data', fontsize=10)\n",
    "axs[1].set_ylabel('Consumo de Mem√≥ria (MB)', fontsize=10)\n",
    "axs[1].legend(loc='upper left', fontsize=10)\n",
    "axs[1].grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "for ax in axs:\n",
    "    ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    ax.tick_params(axis='x', rotation=45, labelsize=9)\n",
    "    ax.tick_params(axis='y', labelsize=9)\n",
    "\n",
    "plt.suptitle('Predi√ß√µes do Transformer Otimizado - Prometheus (MB, Resample 15min)', fontsize=14, y=0.98)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.savefig(os.path.join(DATA_DIR, 'prometheus_transformer_mrfo_15min.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
