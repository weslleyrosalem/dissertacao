{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3318eab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pandas numpy matplotlib seaborn scikit-learn torch prometheus_client prometheus_api prometheus-api-client tsts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "021c2ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.0001, Layers: 2, Heads: 4, FF: 256, MAE: 9.57061767578125, RMSE: 12.145765632628285\n",
      "LR: 0.0001, Layers: 2, Heads: 4, FF: 512, MAE: 10.832914352416992, RMSE: 13.879346561936124\n",
      "LR: 0.0001, Layers: 2, Heads: 8, FF: 256, MAE: 16.180570602416992, RMSE: 24.27762752499073\n",
      "LR: 0.0001, Layers: 2, Heads: 8, FF: 512, MAE: 15.575353622436523, RMSE: 21.443390595802256\n",
      "LR: 0.0001, Layers: 3, Heads: 4, FF: 256, MAE: 14.212325096130371, RMSE: 21.958271332954254\n",
      "LR: 0.0001, Layers: 3, Heads: 4, FF: 512, MAE: 14.257954597473145, RMSE: 17.32384749813597\n",
      "LR: 0.0001, Layers: 3, Heads: 8, FF: 256, MAE: 16.127031326293945, RMSE: 20.47037058231589\n",
      "LR: 0.0001, Layers: 3, Heads: 8, FF: 512, MAE: 19.045679092407227, RMSE: 28.213907867716035\n",
      "LR: 0.0001, Layers: 4, Heads: 4, FF: 256, MAE: 12.373791694641113, RMSE: 16.114498533565396\n",
      "LR: 0.0001, Layers: 4, Heads: 4, FF: 512, MAE: 10.784687995910645, RMSE: 13.609988280886189\n",
      "LR: 0.0001, Layers: 4, Heads: 8, FF: 256, MAE: 22.025236129760742, RMSE: 37.41273864200952\n",
      "LR: 0.0001, Layers: 4, Heads: 8, FF: 512, MAE: 21.80928611755371, RMSE: 30.816420856965102\n",
      "LR: 0.001, Layers: 2, Heads: 4, FF: 256, MAE: 6.801314353942871, RMSE: 7.833364229614795\n",
      "LR: 0.001, Layers: 2, Heads: 4, FF: 512, MAE: 3.777453660964966, RMSE: 4.418488084943435\n",
      "LR: 0.001, Layers: 2, Heads: 8, FF: 256, MAE: 6.532153129577637, RMSE: 9.086794785706292\n",
      "LR: 0.001, Layers: 2, Heads: 8, FF: 512, MAE: 10.054402351379395, RMSE: 12.516487539527551\n",
      "LR: 0.001, Layers: 3, Heads: 4, FF: 256, MAE: 29.26741600036621, RMSE: 41.01214419753846\n",
      "LR: 0.001, Layers: 3, Heads: 4, FF: 512, MAE: 14.44134521484375, RMSE: 18.569772650703836\n",
      "LR: 0.001, Layers: 3, Heads: 8, FF: 256, MAE: 5.13067626953125, RMSE: 6.24438834989127\n",
      "LR: 0.001, Layers: 3, Heads: 8, FF: 512, MAE: 7.664656639099121, RMSE: 10.426365561206742\n",
      "LR: 0.001, Layers: 4, Heads: 4, FF: 256, MAE: 12.377490043640137, RMSE: 15.543011454609593\n",
      "LR: 0.001, Layers: 4, Heads: 4, FF: 512, MAE: 9.79388427734375, RMSE: 13.373580144225533\n",
      "LR: 0.001, Layers: 4, Heads: 8, FF: 256, MAE: 5.479174613952637, RMSE: 6.656280087685079\n",
      "LR: 0.001, Layers: 4, Heads: 8, FF: 512, MAE: 14.237028121948242, RMSE: 22.10480849484451\n",
      "LR: 0.005, Layers: 2, Heads: 4, FF: 256, MAE: 48.329776763916016, RMSE: 59.28541967742258\n",
      "LR: 0.005, Layers: 2, Heads: 4, FF: 512, MAE: 51.2261962890625, RMSE: 61.96157926237436\n",
      "LR: 0.005, Layers: 2, Heads: 8, FF: 256, MAE: 49.08704376220703, RMSE: 60.14945984405222\n",
      "LR: 0.005, Layers: 2, Heads: 8, FF: 512, MAE: 45.485904693603516, RMSE: 58.01233620829065\n",
      "LR: 0.005, Layers: 3, Heads: 4, FF: 256, MAE: 50.12582778930664, RMSE: 61.258053121737596\n",
      "LR: 0.005, Layers: 3, Heads: 4, FF: 512, MAE: 50.07829284667969, RMSE: 60.581040763592036\n",
      "LR: 0.005, Layers: 3, Heads: 8, FF: 256, MAE: 46.35608673095703, RMSE: 58.602199390786524\n",
      "LR: 0.005, Layers: 3, Heads: 8, FF: 512, MAE: 67.84266662597656, RMSE: 78.32737538290812\n",
      "LR: 0.005, Layers: 4, Heads: 4, FF: 256, MAE: 67.20774841308594, RMSE: 77.59451457033705\n",
      "LR: 0.005, Layers: 4, Heads: 4, FF: 512, MAE: 67.70323944091797, RMSE: 78.16348114788805\n",
      "LR: 0.005, Layers: 4, Heads: 8, FF: 256, MAE: 67.45535278320312, RMSE: 77.8782540953346\n",
      "LR: 0.005, Layers: 4, Heads: 8, FF: 512, MAE: 67.81607818603516, RMSE: 78.2936959916394\n",
      "Best Hyperparameters: {'learning_rate': 0.001, 'num_layers': 2, 'nhead': 4, 'dim_feedforward': 512}, Best MAE: 3.777453660964966, Best RMSE: 4.418488084943435\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Definindo constantes\n",
    "DATA_DIR = \"../../data/\"\n",
    "FILE_PATH = os.path.join(DATA_DIR, 'ts.pkl')\n",
    "SEQ_LENGTH = 120  # Aumentado para capturar mais contexto\n",
    "MB = 1_048_576\n",
    "\n",
    "# 1. Carregar e reamostrar os dados\n",
    "df = pd.read_pickle(FILE_PATH)\n",
    "ts = df['value'].astype(float).resample('15min').mean().dropna()  # Testando 15min\n",
    "dates = ts.index\n",
    "\n",
    "# 2. Dividir os dados: 60% treino, 20% validação, 20% teste\n",
    "train_size = int(0.6 * len(ts))\n",
    "val_size = int(0.2 * len(ts))\n",
    "train = ts[:train_size]\n",
    "val = ts[train_size:train_size + val_size]\n",
    "test = ts[train_size + val_size:]\n",
    "\n",
    "# 3. Escalonar os dados\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train.values.reshape(-1, 1))\n",
    "val_scaled = scaler.transform(val.values.reshape(-1, 1))\n",
    "test_scaled = scaler.transform(test.values.reshape(-1, 1))\n",
    "\n",
    "# 4. Criar sequências\n",
    "def create_sequences(data, dates, seq_length):\n",
    "    X, y, y_dates = [], [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i + seq_length])\n",
    "        y.append(data[i + seq_length])\n",
    "        y_dates.append(dates[i + seq_length])\n",
    "    return np.array(X), np.array(y), np.array(y_dates)\n",
    "\n",
    "X_train, y_train, y_dates_train = create_sequences(train_scaled, dates[:train_size], SEQ_LENGTH)\n",
    "X_val, y_val, y_dates_val = create_sequences(val_scaled, dates[train_size:train_size + val_size], SEQ_LENGTH)\n",
    "X_test, y_test, y_dates_test = create_sequences(test_scaled, dates[train_size + val_size:], SEQ_LENGTH)\n",
    "\n",
    "# 5. Ajustar dimensões para o modelo Transformer\n",
    "d_model = 128  # Aumentado para representações mais ricas\n",
    "X_train = np.repeat(X_train, d_model, axis=2)\n",
    "X_val = np.repeat(X_val, d_model, axis=2)\n",
    "X_test = np.repeat(X_test, d_model, axis=2)\n",
    "\n",
    "# 6. Converter para tensores PyTorch\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# 7. Definir codificação posicional\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "# 8. Definir o modelo Transformer\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_layers, dim_feedforward, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.linear_out = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = self.linear_out(output[:, -1, :])  # Pegar apenas o último timestep\n",
    "        return output\n",
    "\n",
    "# Hiperparâmetros para tuning\n",
    "learning_rates = [0.0001, 0.001, 0.005]\n",
    "num_layers_list = [2, 3, 4]\n",
    "nheads = [4, 8]\n",
    "dim_feedforwards = [256, 512]\n",
    "\n",
    "# Outros hiperparâmetros fixos\n",
    "input_dim = d_model\n",
    "batch_size = 32  # Reduzido para maior estabilidade\n",
    "num_epochs = 50  # Aumentado com parada precoce\n",
    "\n",
    "# Função de treinamento\n",
    "def train_model(learning_rate, num_layers, nhead, dim_feedforward):\n",
    "    model = Encoder(input_dim, d_model, nhead, num_layers, dim_feedforward)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            X_batch = X_train[i:i + batch_size]\n",
    "            y_batch = y_train[i:i + batch_size]\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validação\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_val_pred = model(X_val)\n",
    "            val_loss = criterion(y_val_pred, y_val)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "\n",
    "    return model\n",
    "\n",
    "# Grid search\n",
    "best_mae = float('inf')\n",
    "best_rmse = float('inf')\n",
    "best_hyperparams = None\n",
    "best_model = None\n",
    "\n",
    "for lr, nl, nh, df in product(learning_rates, num_layers_list, nheads, dim_feedforwards):\n",
    "    model = train_model(lr, nl, nh, df)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test)\n",
    "    \n",
    "    y_pred_rescaled = scaler.inverse_transform(y_pred.numpy()) / MB\n",
    "    y_test_rescaled = scaler.inverse_transform(y_test.numpy()) / MB\n",
    "    \n",
    "    mae = mean_absolute_error(y_test_rescaled, y_pred_rescaled)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_rescaled, y_pred_rescaled))\n",
    "    \n",
    "    print(f'LR: {lr}, Layers: {nl}, Heads: {nh}, FF: {df}, MAE: {mae}, RMSE: {rmse}')\n",
    "    \n",
    "    if mae < best_mae:\n",
    "        best_mae = mae\n",
    "        best_rmse = rmse\n",
    "        best_hyperparams = {'learning_rate': lr, 'num_layers': nl, 'nhead': nh, 'dim_feedforward': df}\n",
    "        best_model = model\n",
    "\n",
    "print(f'Best Hyperparameters: {best_hyperparams}, Best MAE: {best_mae}, Best RMSE: {best_rmse}')\n",
    "\n",
    "# 9. Fazer previsões\n",
    "model = best_model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_train_pred = model(X_train)\n",
    "    y_test_pred = model(X_test)\n",
    "\n",
    "# 10. Reverter o escalonamento e converter para MB\n",
    "y_train_pred_mb = scaler.inverse_transform(y_train_pred.numpy()) / MB\n",
    "y_train_mb = scaler.inverse_transform(y_train.numpy()) / MB\n",
    "y_test_pred_mb = scaler.inverse_transform(y_test_pred.numpy()) / MB\n",
    "y_test_mb = scaler.inverse_transform(y_test.numpy()) / MB\n",
    "\n",
    "# 11. Preparar dados para plotagem\n",
    "train_df = pd.DataFrame({\n",
    "    'date': y_dates_train,\n",
    "    'actual': y_train_mb.flatten(),\n",
    "    'predicted': y_train_pred_mb.flatten()\n",
    "}).sort_values('date')\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'date': y_dates_test,\n",
    "    'actual': y_test_mb.flatten(),\n",
    "    'predicted': y_test_pred_mb.flatten()\n",
    "}).sort_values('date')\n",
    "\n",
    "# 12. Plotar os resultados\n",
    "plt.style.use('default')\n",
    "fig, axs = plt.subplots(2, 1, figsize=(15, 10), sharex=False)\n",
    "\n",
    "axs[0].plot(train_df['date'], train_df['actual'], label='Real', color='blue', linewidth=1.5)\n",
    "axs[0].plot(train_df['date'], train_df['predicted'], label='Predito', color='red', alpha=0.7, linewidth=1.5)\n",
    "axs[0].set_title('Conjunto de Treinamento (60%)', fontsize=12, pad=10)\n",
    "axs[0].set_ylabel('Consumo de Memória (MB)', fontsize=10)\n",
    "axs[0].legend(loc='upper left', fontsize=10)\n",
    "axs[0].grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "axs[1].plot(test_df['date'], test_df['actual'], label='Real', color='blue', linewidth=1.5)\n",
    "axs[1].plot(test_df['date'], test_df['predicted'], label='Predito', color='red', alpha=0.7, linewidth=1.5)\n",
    "axs[1].set_title('Conjunto de Teste (20%)', fontsize=12, pad=10)\n",
    "axs[1].set_xlabel('Data', fontsize=10)\n",
    "axs[1].set_ylabel('Consumo de Memória (MB)', fontsize=10)\n",
    "axs[1].legend(loc='upper left', fontsize=10)\n",
    "axs[1].grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "for ax in axs:\n",
    "    ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    ax.tick_params(axis='x', rotation=45, labelsize=9)\n",
    "    ax.tick_params(axis='y', labelsize=9)\n",
    "\n",
    "plt.suptitle('Predições do Transformer Otimizado - Prometheus (MB, Resample 15min)', fontsize=14, y=0.98)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.savefig(os.path.join(DATA_DIR, 'prometheus_transformer_optimized_15min.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c47fd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "Best Hyperparameters: {'learning_rate': 0.001, 'num_layers': 4, 'nhead': 8, 'dim_feedforward': 256}, Best MAE: 5.942395210266113, Best RMSE: 8.33626469235449\n",
    "Best Hyperparameters: {'learning_rate': 0.001, 'num_layers': 2, 'nhead': 8, 'dim_feedforward': 256}, Best MAE: 7.6608805656433105, Best RMSE: 11.53275907406166\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
