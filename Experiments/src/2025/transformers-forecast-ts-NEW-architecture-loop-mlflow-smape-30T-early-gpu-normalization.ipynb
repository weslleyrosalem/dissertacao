{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b8d320",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mlflow joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afaafbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/10 20:36:00 INFO mlflow.tracking.fluent: Experiment with name 'Prometheus_Transformer_Experiment_MRFO_RMSE_Optimized-30min' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "\n",
      "Iteration 1/10\n",
      "  Manta 1/30: Fitness (RMSE) = 22.3971\n",
      "  New Best Fitness: 22.3971\n",
      "  Manta 2/30: Fitness (RMSE) = 23.3184\n",
      "  Manta 3/30: Fitness (RMSE) = 27.3766\n",
      "  Manta 4/30: Fitness (RMSE) = 22.3737\n",
      "  New Best Fitness: 22.3737\n",
      "  Manta 5/30: Fitness (RMSE) = 20.4194\n",
      "  New Best Fitness: 20.4194\n",
      "  Manta 6/30: Fitness (RMSE) = 23.1056\n",
      "  Manta 7/30: Fitness (RMSE) = 23.3467\n",
      "  Manta 8/30: Fitness (RMSE) = 25.7966\n",
      "  Manta 9/30: Fitness (RMSE) = 25.0330\n",
      "  Manta 10/30: Fitness (RMSE) = 25.0443\n",
      "  Manta 11/30: Fitness (RMSE) = 25.1085\n",
      "  Manta 12/30: Fitness (RMSE) = 23.5205\n",
      "  Manta 13/30: Fitness (RMSE) = 23.3377\n",
      "  Manta 14/30: Fitness (RMSE) = 22.3227\n",
      "  Manta 15/30: Fitness (RMSE) = 21.6254\n",
      "  Manta 16/30: Fitness (RMSE) = 25.2537\n",
      "  Manta 17/30: Fitness (RMSE) = 26.4234\n",
      "  Manta 18/30: Fitness (RMSE) = 25.8204\n",
      "  Manta 19/30: Fitness (RMSE) = 23.2861\n",
      "  Manta 20/30: Fitness (RMSE) = 22.6920\n",
      "  Manta 21/30: Fitness (RMSE) = 22.0897\n",
      "  Manta 22/30: Fitness (RMSE) = 21.9824\n",
      "  Manta 23/30: Fitness (RMSE) = 24.6763\n",
      "  Manta 24/30: Fitness (RMSE) = 24.5611\n",
      "  Manta 25/30: Fitness (RMSE) = 25.0807\n",
      "  Manta 26/30: Fitness (RMSE) = 21.4645\n",
      "  Manta 27/30: Fitness (RMSE) = 26.4020\n",
      "  Manta 28/30: Fitness (RMSE) = 24.1923\n",
      "  Manta 29/30: Fitness (RMSE) = 25.9096\n",
      "  Manta 30/30: Fitness (RMSE) = 20.8880\n",
      "üèÉ View run MRFO_Iteration_1 at: http://localhost:5001/#/experiments/12/runs/720393ec11c44fc7a9f7ef2a6bd10712\n",
      "üß™ View experiment at: http://localhost:5001/#/experiments/12\n",
      "\n",
      "Early stopping triggered after 1 iterations due to no improvement for 7 iterations.\n",
      "\n",
      "Best MRFO Configuration: LR=0.0009994859795679943, Layers=2, Heads=4, FF=512\n",
      "  Repetition 1/5\n",
      "    MAE: 19.000993728637695, RMSE: 27.756367820553248, MAPE: 2.3600591346621513%, SMAPE: 2.401658296585083%\n",
      "  Repetition 2/5\n",
      "    MAE: 14.754355430603027, RMSE: 23.511042171794646, MAPE: 1.8300456926226616%, SMAPE: 1.8652958869934082%\n",
      "  Repetition 3/5\n",
      "    MAE: 12.166391372680664, RMSE: 20.04312900381754, MAPE: 1.5129968523979187%, SMAPE: 1.5402326583862305%\n",
      "  Repetition 4/5\n",
      "    MAE: 13.404433250427246, RMSE: 19.649511495249776, MAPE: 1.6539037227630615%, SMAPE: 1.6744457483291626%\n",
      "  Repetition 5/5\n",
      "    MAE: 17.59498405456543, RMSE: 29.7499764065689, MAPE: 2.20700241625309%, SMAPE: 2.2591867446899414%\n",
      "  Average MAE: 15.384231567382812 (¬±2.554965202612932), Average RMSE: 24.142005379596824 (¬±4.047081869900135)\n",
      "  Average MAPE: 1.9128015637397766% (¬±0.3225876345568276), Average SMAPE: 1.9481639862060547% (¬±0.33184003829956055)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/05/10 20:40:36 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run Best_MRFO_Run at: http://localhost:5001/#/experiments/12/runs/7597958678954c668e273a3ea8fc54d8\n",
      "üß™ View experiment at: http://localhost:5001/#/experiments/12\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "# Configurar MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5001\")\n",
    "mlflow.set_experiment(\"Prometheus_Transformer_Experiment_MRFO_RMSE_Optimized-30min\")\n",
    "\n",
    "# Definir o dispositivo (usar MPS para Apple Silicon GPU se dispon√≠vel)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Definindo constantes\n",
    "DATA_DIR = \"/Users/wrosalem/Documents/0x0-Coding/GitHub/dissertacao/Experiments/data/\"\n",
    "FILE_PATH = os.path.join(DATA_DIR, 'ts.pkl')\n",
    "SEQ_LENGTH = 24  # 12 horas (24 * 30min)\n",
    "MB = 1_048_576\n",
    "\n",
    "# 1. Carregar e reamostrar os dados para 30 minutos\n",
    "df = pd.read_pickle(FILE_PATH)\n",
    "ts = df['value'].astype(float)\n",
    "# Aplicar suaviza√ß√£o com m√©dia m√≥vel (window=3)\n",
    "ts = ts.rolling(window=3, min_periods=1).mean()\n",
    "ts = ts.resample('30min').mean().dropna()\n",
    "dates = ts.index\n",
    "\n",
    "# 2. Dividir os dados: 60% treino, 20% valida√ß√£o, 20% teste\n",
    "train_size = int(0.6 * len(ts))\n",
    "val_size = int(0.2 * len(ts))\n",
    "train = ts[:train_size]\n",
    "val = ts[train_size:train_size + val_size]\n",
    "test = ts[train_size + val_size:]\n",
    "\n",
    "# 3. Escalonar os dados\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train.values.reshape(-1, 1))\n",
    "val_scaled = scaler.transform(val.values.reshape(-1, 1))\n",
    "test_scaled = scaler.transform(test.values.reshape(-1, 1))\n",
    "\n",
    "# 4. Criar sequ√™ncias\n",
    "def create_sequences(data, dates, seq_length):\n",
    "    X, y, y_dates = [], [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i + seq_length])\n",
    "        y.append(data[i + seq_length])\n",
    "        y_dates.append(dates[i + seq_length])\n",
    "    return np.array(X), np.array(y), np.array(y_dates)\n",
    "\n",
    "X_train, y_train, y_dates_train = create_sequences(train_scaled, dates[:train_size], SEQ_LENGTH)\n",
    "X_val, y_val, y_dates_val = create_sequences(val_scaled, dates[train_size:train_size + val_size], SEQ_LENGTH)\n",
    "X_test, y_test, y_dates_test = create_sequences(test_scaled, dates[train_size + val_size:], SEQ_LENGTH)\n",
    "\n",
    "# 5. Ajustar dimens√µes para o modelo Transformer\n",
    "d_model = 128\n",
    "X_train = np.repeat(X_train, d_model, axis=2)\n",
    "X_val = np.repeat(X_val, d_model, axis=2)\n",
    "X_test = np.repeat(X_test, d_model, axis=2)\n",
    "\n",
    "# 6. Converter para tensores PyTorch e mover para o dispositivo\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# 7. Definir codifica√ß√£o posicional\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "# 8. Definir o modelo Transformer com dropout\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_layers, dim_feedforward, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_out = nn.Linear(d_model, 1, bias=True)\n",
    "        # Inicializar pesos com Xavier\n",
    "        nn.init.xavier_uniform_(self.linear_out.weight)\n",
    "        if self.linear_out.bias is not None:\n",
    "            nn.init.zeros_(self.linear_out.bias)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = self.dropout(output[:, -1, :])\n",
    "        output = self.linear_out(output)\n",
    "        return output\n",
    "\n",
    "# Outros hiperpar√¢metros fixos\n",
    "input_dim = d_model\n",
    "batch_size = 64  # Aumentado para maior estabilidade\n",
    "num_epochs = 50\n",
    "\n",
    "# Fun√ß√£o para calcular SMAPE\n",
    "def smape(y_true, y_pred):\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    return 100 * np.mean(diff)\n",
    "\n",
    "# Fun√ß√£o de treinamento e avalia√ß√£o para uma repeti√ß√£o\n",
    "def train_and_evaluate(learning_rate):\n",
    "    # Fixar hiperpar√¢metros conhecidos\n",
    "    num_layers = 2\n",
    "    nhead = 4\n",
    "    dim_feedforward = 512\n",
    "    \n",
    "    model = Encoder(input_dim, d_model, nhead, num_layers, dim_feedforward, dropout=0.1).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            X_batch = X_train[i:i + batch_size]\n",
    "            y_batch = y_train[i:i + batch_size]\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test)\n",
    "    \n",
    "    # Mover os dados de volta para a CPU para c√°lculos de m√©tricas\n",
    "    y_pred_rescaled = scaler.inverse_transform(y_pred.cpu().numpy()) / MB\n",
    "    y_test_rescaled = scaler.inverse_transform(y_test.cpu().numpy()) / MB\n",
    "    \n",
    "    mae = mean_absolute_error(y_test_rescaled, y_pred_rescaled)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_rescaled, y_pred_rescaled))\n",
    "    mape = mean_absolute_percentage_error(y_test_rescaled, y_pred_rescaled) * 100\n",
    "    smape_val = smape(y_test_rescaled, y_pred_rescaled)\n",
    "    \n",
    "    return mae, rmse, mape, smape_val, model\n",
    "\n",
    "# Fun√ß√£o de mapeamento de valores cont√≠nuos para discretos\n",
    "def map_continuous_to_discrete(value, discrete_values):\n",
    "    idx = int(round(value * (len(discrete_values) - 1)))\n",
    "    idx = max(0, min(idx, len(discrete_values) - 1))\n",
    "    return discrete_values[idx]\n",
    "\n",
    "# Fun√ß√£o de avalia√ß√£o para uma manta (usada no MRFO)\n",
    "def evaluate_manta(params, n_repetitions):\n",
    "    lr = 10 ** params[0]  # learning_rate (log scale)\n",
    "    \n",
    "    # Executar repeti√ß√µes sequencialmente (GPU n√£o se beneficia de paralelismo aqui)\n",
    "    results = [train_and_evaluate(lr) for _ in range(n_repetitions)]\n",
    "    rmse_list = [result[1] for result in results]  # Minimizar RMSE\n",
    "    return np.mean(rmse_list)\n",
    "\n",
    "# Implementa√ß√£o do MRFO com sa√≠das intermedi√°rias e early stopping\n",
    "class MRFO:\n",
    "    def __init__(self, objective_func, bounds, n_mantas=30, max_iter=100, patience=7, n_repetitions=5):\n",
    "        self.objective_func = objective_func\n",
    "        self.bounds = np.array(bounds).T  # Shape: (2, dim)\n",
    "        self.n_mantas = n_mantas\n",
    "        self.max_iter = max_iter\n",
    "        self.patience = patience\n",
    "        self.n_repetitions = n_repetitions\n",
    "        self.dim = self.bounds.shape[1]\n",
    "        \n",
    "        # Inicializar popula√ß√£o\n",
    "        self.positions = np.zeros((self.n_mantas, self.dim))\n",
    "        for d in range(self.dim):\n",
    "            self.positions[:, d] = np.random.uniform(self.bounds[0, d], self.bounds[1, d], self.n_mantas)\n",
    "        self.fitness = np.array([float('inf')] * self.n_mantas)\n",
    "        self.best_position = None\n",
    "        self.best_fitness = float('inf')\n",
    "        self.no_improvement_count = 0  # Contador para early stopping\n",
    "\n",
    "    def optimize(self):\n",
    "        for t in range(self.max_iter):\n",
    "            print(f\"\\nIteration {t+1}/{self.max_iter}\")\n",
    "            # Avaliar fitness de todas as mantas sequencialmente (GPU n√£o se beneficia de paralelismo aqui)\n",
    "            fitness_results = [self.objective_func(self.positions[i], self.n_repetitions) for i in range(self.n_mantas)]\n",
    "            self.fitness = np.array(fitness_results)\n",
    "\n",
    "            # Exibir resultados e atualizar o melhor fitness\n",
    "            for i in range(self.n_mantas):\n",
    "                print(f\"  Manta {i+1}/{self.n_mantas}: Fitness (RMSE) = {self.fitness[i]:.4f}\")\n",
    "                if self.fitness[i] < self.best_fitness:\n",
    "                    self.best_fitness = self.fitness[i]\n",
    "                    self.best_position = self.positions[i].copy()\n",
    "                    self.no_improvement_count = 0  # Resetar o contador\n",
    "                    print(f\"  New Best Fitness: {self.best_fitness:.4f}\")\n",
    "                else:\n",
    "                    self.no_improvement_count += 1\n",
    "\n",
    "            # Registrar melhor fitness no MLflow\n",
    "            with mlflow.start_run(run_name=f\"MRFO_Iteration_{t+1}\"):\n",
    "                mlflow.log_metric(\"best_fitness_rmse\", self.best_fitness)\n",
    "                # Registrar os hiperpar√¢metros correspondentes ao melhor fitness\n",
    "                lr = 10 ** self.best_position[0]\n",
    "                mlflow.log_param(\"learning_rate\", lr)\n",
    "                mlflow.log_param(\"num_layers\", 2)\n",
    "                mlflow.log_param(\"nhead\", 4)\n",
    "                mlflow.log_param(\"dim_feedforward\", 512)\n",
    "\n",
    "            # Crit√©rio de parada precoce\n",
    "            if self.no_improvement_count >= self.patience:\n",
    "                print(f\"\\nEarly stopping triggered after {t+1} iterations due to no improvement for {self.patience} iterations.\")\n",
    "                break\n",
    "\n",
    "            # Atualizar posi√ß√µes usando Chain Foraging, Cyclone Foraging e Somersault Foraging\n",
    "            for i in range(self.n_mantas):\n",
    "                r = np.random.random(self.dim)\n",
    "                r1 = np.random.random()\n",
    "\n",
    "                # Chain Foraging\n",
    "                if r1 < 0.5:\n",
    "                    if i == 0:\n",
    "                        self.positions[i] = self.positions[i] + r * (self.best_position - self.positions[i]) + \\\n",
    "                                            r * (self.best_position - self.positions[i])\n",
    "                    else:\n",
    "                        self.positions[i] = self.positions[i] + r * (self.positions[i-1] - self.positions[i]) + \\\n",
    "                                            r * (self.best_position - self.positions[i])\n",
    "\n",
    "                # Cyclone Foraging\n",
    "                else:\n",
    "                    beta = 2 * np.exp(r1 * (self.max_iter - t + 1) / self.max_iter) * np.sin(2 * np.pi * r1)\n",
    "                    if r1 < 0.5:\n",
    "                        self.positions[i] = self.positions[i] + r * (self.best_position - beta * self.positions[i])\n",
    "                    else:\n",
    "                        idx = np.random.randint(0, self.n_mantas)\n",
    "                        self.positions[i] = self.positions[i] + r * (self.positions[idx] - beta * self.positions[i])\n",
    "\n",
    "                # Somersault Foraging\n",
    "                r2 = np.random.random()\n",
    "                self.positions[i] = self.positions[i] + 0.5 * (self.best_position + self.positions[i]) * (2 * r2 - 1)\n",
    "\n",
    "                # Garantir que as posi√ß√µes estejam dentro dos limites\n",
    "                self.positions[i] = np.clip(self.positions[i], self.bounds[0], self.bounds[1])\n",
    "\n",
    "        return self.best_position, self.best_fitness\n",
    "\n",
    "# MRFO para otimizar hiperpar√¢metros\n",
    "n_repetitions = 5\n",
    "bounds = [\n",
    "    [-3.0458, -2.9586],  # log10(learning_rate): [0.0009, 0.0011]\n",
    "]\n",
    "\n",
    "mrfo = MRFO(lambda params, reps: evaluate_manta(params, reps), bounds, n_mantas=30, max_iter=10, patience=7, n_repetitions=n_repetitions)\n",
    "best_position, best_fitness = mrfo.optimize()\n",
    "\n",
    "# Mapear a melhor posi√ß√£o para hiperpar√¢metros\n",
    "best_lr = 10 ** best_position[0]\n",
    "\n",
    "# Treinar o modelo com a melhor configura√ß√£o para obter m√©tricas finais\n",
    "with mlflow.start_run(run_name=\"Best_MRFO_Run\"):\n",
    "    # Registrar hiperpar√¢metros\n",
    "    mlflow.log_param(\"learning_rate\", best_lr)\n",
    "    mlflow.log_param(\"num_layers\", 2)\n",
    "    mlflow.log_param(\"nhead\", 4)\n",
    "    mlflow.log_param(\"dim_feedforward\", 512)\n",
    "    mlflow.log_param(\"seq_length\", SEQ_LENGTH)\n",
    "    mlflow.log_param(\"resample_interval\", \"30min\")\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
    "\n",
    "    print(f\"\\nBest MRFO Configuration: LR={best_lr}, Layers=2, Heads=4, FF=512\")\n",
    "    \n",
    "    # Executar repeti√ß√µes sequencialmente (GPU n√£o se beneficia de paralelismo aqui)\n",
    "    results = [train_and_evaluate(best_lr) for _ in range(n_repetitions)]\n",
    "\n",
    "    mae_list = [result[0] for result in results]\n",
    "    rmse_list = [result[1] for result in results]\n",
    "    mape_list = [result[2] for result in results]\n",
    "    smape_list = [result[3] for result in results]\n",
    "    models = [result[4] for result in results]\n",
    "\n",
    "    for rep, (mae, rmse, mape, smape_val, _) in enumerate(results):\n",
    "        print(f\"  Repetition {rep+1}/{n_repetitions}\")\n",
    "        print(f\"    MAE: {mae}, RMSE: {rmse}, MAPE: {mape}%, SMAPE: {smape_val}%\")\n",
    "\n",
    "    avg_mae = np.mean(mae_list)\n",
    "    avg_rmse = np.mean(rmse_list)\n",
    "    avg_mape = np.mean(mape_list)\n",
    "    avg_smape = np.mean(smape_list)\n",
    "    std_mae = np.std(mae_list)\n",
    "    std_rmse = np.std(rmse_list)\n",
    "    std_mape = np.std(mape_list)\n",
    "    std_smape = np.std(smape_list)\n",
    "\n",
    "    print(f\"  Average MAE: {avg_mae} (¬±{std_mae}), Average RMSE: {avg_rmse} (¬±{std_rmse})\")\n",
    "    print(f\"  Average MAPE: {avg_mape}% (¬±{std_mape}), Average SMAPE: {avg_smape}% (¬±{std_smape})\")\n",
    "\n",
    "    # Registrar m√©tricas no MLflow\n",
    "    mlflow.log_metric(\"avg_mae\", avg_mae)\n",
    "    mlflow.log_metric(\"std_mae\", std_mae)\n",
    "    mlflow.log_metric(\"avg_rmse\", avg_rmse)\n",
    "    mlflow.log_metric(\"std_rmse\", std_rmse)\n",
    "    mlflow.log_metric(\"avg_mape\", avg_mape)\n",
    "    mlflow.log_metric(\"std_mape\", std_mape)\n",
    "    mlflow.log_metric(\"avg_smape\", avg_smape)\n",
    "    mlflow.log_metric(\"std_smape\", std_smape)\n",
    "\n",
    "    best_model = models[0]\n",
    "    mlflow.pytorch.log_model(best_model, \"best_model\")\n",
    "\n",
    "# 9. Fazer previs√µes\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_train_pred = best_model(X_train)\n",
    "    y_test_pred = best_model(X_test)\n",
    "\n",
    "# 10. Reverter o escalonamento e converter para MB\n",
    "y_train_pred_mb = scaler.inverse_transform(y_train_pred.cpu().numpy()) / MB\n",
    "y_train_mb = scaler.inverse_transform(y_train.cpu().numpy()) / MB\n",
    "y_test_pred_mb = scaler.inverse_transform(y_test_pred.cpu().numpy()) / MB\n",
    "y_test_mb = scaler.inverse_transform(y_test.cpu().numpy()) / MB\n",
    "\n",
    "# 11. Preparar dados para plotagem\n",
    "train_df = pd.DataFrame({\n",
    "    'date': y_dates_train,\n",
    "    'actual': y_train_mb.flatten(),\n",
    "    'predicted': y_train_pred_mb.flatten()\n",
    "}).sort_values('date')\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'date': y_dates_test,\n",
    "    'actual': y_test_mb.flatten(),\n",
    "    'predicted': y_test_pred_mb.flatten()\n",
    "}).sort_values('date')\n",
    "\n",
    "# 12. Plotar os resultados\n",
    "plt.style.use('default')\n",
    "fig, axs = plt.subplots(2, 1, figsize=(15, 10), sharex=False)\n",
    "\n",
    "axs[0].plot(train_df['date'], train_df['actual'], label='Real', color='blue', linewidth=1.5)\n",
    "axs[0].plot(train_df['date'], train_df['predicted'], label='Predito', color='red', alpha=0.7, linewidth=1.5)\n",
    "axs[0].set_title('Conjunto de Treinamento (60%)', fontsize=12, pad=10)\n",
    "axs[0].set_ylabel('Consumo de Mem√≥ria (MB)', fontsize=10)\n",
    "axs[0].legend(loc='upper left', fontsize=10)\n",
    "axs[0].grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "axs[1].plot(test_df['date'], test_df['actual'], label='Real', color='blue', linewidth=1.5)\n",
    "axs[1].plot(test_df['date'], test_df['predicted'], label='Predito', color='red', alpha=0.7, linewidth=1.5)\n",
    "axs[1].set_title('Conjunto de Teste (20%)', fontsize=12, pad=10)\n",
    "axs[1].set_xlabel('Data', fontsize=10)\n",
    "axs[1].set_ylabel('Consumo de Mem√≥ria (MB)', fontsize=10)\n",
    "axs[1].legend(loc='upper left', fontsize=10)\n",
    "axs[1].grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "for ax in axs:\n",
    "    ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    ax.tick_params(axis='x', rotation=45, labelsize=9)\n",
    "    ax.tick_params(axis='y', labelsize=9)\n",
    "\n",
    "plt.suptitle('Predi√ß√µes do Transformer Otimizado - Prometheus (MB, Resample 30min)', fontsize=14, y=0.98)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.savefig(os.path.join(DATA_DIR, 'prometheus_transformer_mrfo_30min.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
