{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b8d320",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mlflow joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afaafbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "\n",
      "Iteration 1/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 274\u001b[39m\n\u001b[32m    269\u001b[39m bounds = [\n\u001b[32m    270\u001b[39m     [-\u001b[32m3.0458\u001b[39m, -\u001b[32m2.9586\u001b[39m],  \u001b[38;5;66;03m# log10(learning_rate): [0.0009, 0.0011]\u001b[39;00m\n\u001b[32m    271\u001b[39m ]\n\u001b[32m    273\u001b[39m mrfo = MRFO(\u001b[38;5;28;01mlambda\u001b[39;00m params, reps: evaluate_manta(params, reps), bounds, n_mantas=\u001b[32m30\u001b[39m, max_iter=\u001b[32m10\u001b[39m, patience=\u001b[32m7\u001b[39m, n_repetitions=n_repetitions)\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m best_position, best_fitness = \u001b[43mmrfo\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[38;5;66;03m# Mapear a melhor posição para hiperparâmetros\u001b[39;00m\n\u001b[32m    277\u001b[39m best_lr = \u001b[32m10\u001b[39m ** best_position[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 206\u001b[39m, in \u001b[36mMRFO.optimize\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.max_iter\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    205\u001b[39m \u001b[38;5;66;03m# Avaliar fitness de todas as mantas sequencialmente (GPU não se beneficia de paralelismo aqui)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m fitness_results = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobjective_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_repetitions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_mantas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[38;5;28mself\u001b[39m.fitness = np.array(fitness_results)\n\u001b[32m    209\u001b[39m \u001b[38;5;66;03m# Exibir resultados e atualizar o melhor fitness\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 206\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.max_iter\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    205\u001b[39m \u001b[38;5;66;03m# Avaliar fitness de todas as mantas sequencialmente (GPU não se beneficia de paralelismo aqui)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m fitness_results = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobjective_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_repetitions\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.n_mantas)]\n\u001b[32m    207\u001b[39m \u001b[38;5;28mself\u001b[39m.fitness = np.array(fitness_results)\n\u001b[32m    209\u001b[39m \u001b[38;5;66;03m# Exibir resultados e atualizar o melhor fitness\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 273\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(params, reps)\u001b[39m\n\u001b[32m    268\u001b[39m n_repetitions = \u001b[32m5\u001b[39m\n\u001b[32m    269\u001b[39m bounds = [\n\u001b[32m    270\u001b[39m     [-\u001b[32m3.0458\u001b[39m, -\u001b[32m2.9586\u001b[39m],  \u001b[38;5;66;03m# log10(learning_rate): [0.0009, 0.0011]\u001b[39;00m\n\u001b[32m    271\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m mrfo = MRFO(\u001b[38;5;28;01mlambda\u001b[39;00m params, reps: \u001b[43mevaluate_manta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreps\u001b[49m\u001b[43m)\u001b[49m, bounds, n_mantas=\u001b[32m30\u001b[39m, max_iter=\u001b[32m10\u001b[39m, patience=\u001b[32m7\u001b[39m, n_repetitions=n_repetitions)\n\u001b[32m    274\u001b[39m best_position, best_fitness = mrfo.optimize()\n\u001b[32m    276\u001b[39m \u001b[38;5;66;03m# Mapear a melhor posição para hiperparâmetros\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 178\u001b[39m, in \u001b[36mevaluate_manta\u001b[39m\u001b[34m(params, n_repetitions)\u001b[39m\n\u001b[32m    175\u001b[39m lr = \u001b[32m10\u001b[39m ** params[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# learning_rate (log scale)\u001b[39;00m\n\u001b[32m    177\u001b[39m \u001b[38;5;66;03m# Executar repetições sequencialmente (GPU não se beneficia de paralelismo aqui)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m results = \u001b[43m[\u001b[49m\u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_repetitions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    179\u001b[39m rmse_list = [result[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results]  \u001b[38;5;66;03m# Minimizar RMSE\u001b[39;00m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.mean(rmse_list)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 178\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    175\u001b[39m lr = \u001b[32m10\u001b[39m ** params[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# learning_rate (log scale)\u001b[39;00m\n\u001b[32m    177\u001b[39m \u001b[38;5;66;03m# Executar repetições sequencialmente (GPU não se beneficia de paralelismo aqui)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m results = [\u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_repetitions)]\n\u001b[32m    179\u001b[39m rmse_list = [result[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results]  \u001b[38;5;66;03m# Minimizar RMSE\u001b[39;00m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.mean(rmse_list)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 148\u001b[39m, in \u001b[36mtrain_and_evaluate\u001b[39m\u001b[34m(learning_rate)\u001b[39m\n\u001b[32m    146\u001b[39m     loss.backward()\n\u001b[32m    147\u001b[39m     \u001b[38;5;66;03m# Gradient clipping\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m     optimizer.step()\n\u001b[32m    150\u001b[39m scheduler.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/0x0-Coding/GitHub/dissertacao/Experiments/src/2025/.venv/lib/python3.11/site-packages/torch/nn/utils/clip_grad.py:38\u001b[39m, in \u001b[36m_no_grad.<locals>._no_grad_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_no_grad_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/0x0-Coding/GitHub/dissertacao/Experiments/src/2025/.venv/lib/python3.11/site-packages/torch/nn/utils/clip_grad.py:219\u001b[39m, in \u001b[36mclip_grad_norm_\u001b[39m\u001b[34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[39m\n\u001b[32m    217\u001b[39m     parameters = \u001b[38;5;28mlist\u001b[39m(parameters)\n\u001b[32m    218\u001b[39m grads = [p.grad \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters \u001b[38;5;28;01mif\u001b[39;00m p.grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m total_norm = \u001b[43m_get_total_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_if_nonfinite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m _clip_grads_with_norm_(parameters, max_norm, total_norm, foreach)\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m total_norm\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/0x0-Coding/GitHub/dissertacao/Experiments/src/2025/.venv/lib/python3.11/site-packages/torch/nn/utils/clip_grad.py:38\u001b[39m, in \u001b[36m_no_grad.<locals>._no_grad_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_no_grad_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/0x0-Coding/GitHub/dissertacao/Experiments/src/2025/.venv/lib/python3.11/site-packages/torch/nn/utils/clip_grad.py:98\u001b[39m, in \u001b[36m_get_total_norm\u001b[39m\u001b[34m(tensors, norm_type, error_if_nonfinite, foreach)\u001b[39m\n\u001b[32m     93\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     94\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mforeach=True was passed, but can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt use the foreach API on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice.type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tensors\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m         )\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     97\u001b[39m         norms.extend(\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m             \u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvector_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdevice_tensors\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     99\u001b[39m         )\n\u001b[32m    101\u001b[39m total_norm = torch.linalg.vector_norm(\n\u001b[32m    102\u001b[39m     torch.stack([norm.to(first_device) \u001b[38;5;28;01mfor\u001b[39;00m norm \u001b[38;5;129;01min\u001b[39;00m norms]), norm_type\n\u001b[32m    103\u001b[39m )\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_if_nonfinite \u001b[38;5;129;01mand\u001b[39;00m torch.logical_or(total_norm.isnan(), total_norm.isinf()):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/0x0-Coding/GitHub/dissertacao/Experiments/src/2025/.venv/lib/python3.11/site-packages/torch/nn/utils/clip_grad.py:98\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     93\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     94\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mforeach=True was passed, but can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt use the foreach API on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice.type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tensors\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m         )\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     97\u001b[39m         norms.extend(\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m             [\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvector_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m device_tensors]\n\u001b[32m     99\u001b[39m         )\n\u001b[32m    101\u001b[39m total_norm = torch.linalg.vector_norm(\n\u001b[32m    102\u001b[39m     torch.stack([norm.to(first_device) \u001b[38;5;28;01mfor\u001b[39;00m norm \u001b[38;5;129;01min\u001b[39;00m norms]), norm_type\n\u001b[32m    103\u001b[39m )\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_if_nonfinite \u001b[38;5;129;01mand\u001b[39;00m torch.logical_or(total_norm.isnan(), total_norm.isinf()):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "# Configurar MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5001\")\n",
    "mlflow.set_experiment(\"Prometheus_Transformer_Experiment_MRFO_RMSE_Optimized\")\n",
    "\n",
    "# Definir o dispositivo (usar MPS para Apple Silicon GPU se disponível)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Definindo constantes\n",
    "DATA_DIR = \"../../data/\"\n",
    "FILE_PATH = os.path.join(DATA_DIR, 'ts.pkl')\n",
    "SEQ_LENGTH = 720  # 12 horas (720 * 1min)\n",
    "MB = 1_048_576\n",
    "\n",
    "# 1. Carregar e reamostrar os dados para 1 minuto\n",
    "df = pd.read_pickle(FILE_PATH)\n",
    "ts = df['value'].astype(float)\n",
    "# Aplicar suavização com média móvel (window=3)\n",
    "ts = ts.rolling(window=3, min_periods=1).mean()\n",
    "ts = ts.resample('1min').mean().dropna()\n",
    "dates = ts.index\n",
    "\n",
    "# 2. Dividir os dados: 60% treino, 20% validação, 20% teste\n",
    "train_size = int(0.6 * len(ts))\n",
    "val_size = int(0.2 * len(ts))\n",
    "train = ts[:train_size]\n",
    "val = ts[train_size:train_size + val_size]\n",
    "test = ts[train_size + val_size:]\n",
    "\n",
    "# 3. Escalonar os dados\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train.values.reshape(-1, 1))\n",
    "val_scaled = scaler.transform(val.values.reshape(-1, 1))\n",
    "test_scaled = scaler.transform(test.values.reshape(-1, 1))\n",
    "\n",
    "# 4. Criar sequências\n",
    "def create_sequences(data, dates, seq_length):\n",
    "    X, y, y_dates = [], [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i + seq_length])\n",
    "        y.append(data[i + seq_length])\n",
    "        y_dates.append(dates[i + seq_length])\n",
    "    return np.array(X), np.array(y), np.array(y_dates)\n",
    "\n",
    "X_train, y_train, y_dates_train = create_sequences(train_scaled, dates[:train_size], SEQ_LENGTH)\n",
    "X_val, y_val, y_dates_val = create_sequences(val_scaled, dates[train_size:train_size + val_size], SEQ_LENGTH)\n",
    "X_test, y_test, y_dates_test = create_sequences(test_scaled, dates[train_size + val_size:], SEQ_LENGTH)\n",
    "\n",
    "# 5. Ajustar dimensões para o modelo Transformer\n",
    "d_model = 128\n",
    "X_train = np.repeat(X_train, d_model, axis=2)\n",
    "X_val = np.repeat(X_val, d_model, axis=2)\n",
    "X_test = np.repeat(X_test, d_model, axis=2)\n",
    "\n",
    "# 6. Converter para tensores PyTorch e mover para o dispositivo\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# 7. Definir codificação posicional\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "# 8. Definir o modelo Transformer com dropout\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_layers, dim_feedforward, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_out = nn.Linear(d_model, 1, bias=True)\n",
    "        # Inicializar pesos com Xavier\n",
    "        nn.init.xavier_uniform_(self.linear_out.weight)\n",
    "        if self.linear_out.bias is not None:\n",
    "            nn.init.zeros_(self.linear_out.bias)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = self.dropout(output[:, -1, :])\n",
    "        output = self.linear_out(output)\n",
    "        return output\n",
    "\n",
    "# Outros hiperparâmetros fixos\n",
    "input_dim = d_model\n",
    "batch_size = 64  # Aumentado para maior estabilidade\n",
    "num_epochs = 50\n",
    "\n",
    "# Função para calcular SMAPE\n",
    "def smape(y_true, y_pred):\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    return 100 * np.mean(diff)\n",
    "\n",
    "# Função de treinamento e avaliação para uma repetição\n",
    "def train_and_evaluate(learning_rate):\n",
    "    # Fixar hiperparâmetros conhecidos\n",
    "    num_layers = 2\n",
    "    nhead = 4\n",
    "    dim_feedforward = 512\n",
    "    \n",
    "    model = Encoder(input_dim, d_model, nhead, num_layers, dim_feedforward, dropout=0.1).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            X_batch = X_train[i:i + batch_size]\n",
    "            y_batch = y_train[i:i + batch_size]\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test)\n",
    "    \n",
    "    # Mover os dados de volta para a CPU para cálculos de métricas\n",
    "    y_pred_rescaled = scaler.inverse_transform(y_pred.cpu().numpy()) / MB\n",
    "    y_test_rescaled = scaler.inverse_transform(y_test.cpu().numpy()) / MB\n",
    "    \n",
    "    mae = mean_absolute_error(y_test_rescaled, y_pred_rescaled)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_rescaled, y_pred_rescaled))\n",
    "    mape = mean_absolute_percentage_error(y_test_rescaled, y_pred_rescaled) * 100\n",
    "    smape_val = smape(y_test_rescaled, y_pred_rescaled)\n",
    "    \n",
    "    return mae, rmse, mape, smape_val, model\n",
    "\n",
    "# Função de mapeamento de valores contínuos para discretos\n",
    "def map_continuous_to_discrete(value, discrete_values):\n",
    "    idx = int(round(value * (len(discrete_values) - 1)))\n",
    "    idx = max(0, min(idx, len(discrete_values) - 1))\n",
    "    return discrete_values[idx]\n",
    "\n",
    "# Função de avaliação para uma manta (usada no MRFO)\n",
    "def evaluate_manta(params, n_repetitions):\n",
    "    lr = 10 ** params[0]  # learning_rate (log scale)\n",
    "    \n",
    "    # Executar repetições sequencialmente (GPU não se beneficia de paralelismo aqui)\n",
    "    results = [train_and_evaluate(lr) for _ in range(n_repetitions)]\n",
    "    rmse_list = [result[1] for result in results]  # Minimizar RMSE\n",
    "    return np.mean(rmse_list)\n",
    "\n",
    "# Implementação do MRFO com saídas intermediárias e early stopping\n",
    "class MRFO:\n",
    "    def __init__(self, objective_func, bounds, n_mantas=30, max_iter=100, patience=7, n_repetitions=5):\n",
    "        self.objective_func = objective_func\n",
    "        self.bounds = np.array(bounds).T  # Shape: (2, dim)\n",
    "        self.n_mantas = n_mantas\n",
    "        self.max_iter = max_iter\n",
    "        self.patience = patience\n",
    "        self.n_repetitions = n_repetitions\n",
    "        self.dim = self.bounds.shape[1]\n",
    "        \n",
    "        # Inicializar população\n",
    "        self.positions = np.zeros((self.n_mantas, self.dim))\n",
    "        for d in range(self.dim):\n",
    "            self.positions[:, d] = np.random.uniform(self.bounds[0, d], self.bounds[1, d], self.n_mantas)\n",
    "        self.fitness = np.array([float('inf')] * self.n_mantas)\n",
    "        self.best_position = None\n",
    "        self.best_fitness = float('inf')\n",
    "        self.no_improvement_count = 0  # Contador para early stopping\n",
    "\n",
    "    def optimize(self):\n",
    "        for t in range(self.max_iter):\n",
    "            print(f\"\\nIteration {t+1}/{self.max_iter}\")\n",
    "            # Avaliar fitness de todas as mantas sequencialmente (GPU não se beneficia de paralelismo aqui)\n",
    "            fitness_results = [self.objective_func(self.positions[i], self.n_repetitions) for i in range(self.n_mantas)]\n",
    "            self.fitness = np.array(fitness_results)\n",
    "\n",
    "            # Exibir resultados e atualizar o melhor fitness\n",
    "            for i in range(self.n_mantas):\n",
    "                print(f\"  Manta {i+1}/{self.n_mantas}: Fitness (RMSE) = {self.fitness[i]:.4f}\")\n",
    "                if self.fitness[i] < self.best_fitness:\n",
    "                    self.best_fitness = self.fitness[i]\n",
    "                    self.best_position = self.positions[i].copy()\n",
    "                    self.no_improvement_count = 0  # Resetar o contador\n",
    "                    print(f\"  New Best Fitness: {self.best_fitness:.4f}\")\n",
    "                else:\n",
    "                    self.no_improvement_count += 1\n",
    "\n",
    "            # Registrar melhor fitness no MLflow\n",
    "            with mlflow.start_run(run_name=f\"MRFO_Iteration_{t+1}\"):\n",
    "                mlflow.log_metric(\"best_fitness_rmse\", self.best_fitness)\n",
    "                # Registrar os hiperparâmetros correspondentes ao melhor fitness\n",
    "                lr = 10 ** self.best_position[0]\n",
    "                mlflow.log_param(\"learning_rate\", lr)\n",
    "                mlflow.log_param(\"num_layers\", 2)\n",
    "                mlflow.log_param(\"nhead\", 4)\n",
    "                mlflow.log_param(\"dim_feedforward\", 512)\n",
    "\n",
    "            # Critério de parada precoce\n",
    "            if self.no_improvement_count >= self.patience:\n",
    "                print(f\"\\nEarly stopping triggered after {t+1} iterations due to no improvement for {self.patience} iterations.\")\n",
    "                break\n",
    "\n",
    "            # Atualizar posições usando Chain Foraging, Cyclone Foraging e Somersault Foraging\n",
    "            for i in range(self.n_mantas):\n",
    "                r = np.random.random(self.dim)\n",
    "                r1 = np.random.random()\n",
    "\n",
    "                # Chain Foraging\n",
    "                if r1 < 0.5:\n",
    "                    if i == 0:\n",
    "                        self.positions[i] = self.positions[i] + r * (self.best_position - self.positions[i]) + \\\n",
    "                                            r * (self.best_position - self.positions[i])\n",
    "                    else:\n",
    "                        self.positions[i] = self.positions[i] + r * (self.positions[i-1] - self.positions[i]) + \\\n",
    "                                            r * (self.best_position - self.positions[i])\n",
    "\n",
    "                # Cyclone Foraging\n",
    "                else:\n",
    "                    beta = 2 * np.exp(r1 * (self.max_iter - t + 1) / self.max_iter) * np.sin(2 * np.pi * r1)\n",
    "                    if r1 < 0.5:\n",
    "                        self.positions[i] = self.positions[i] + r * (self.best_position - beta * self.positions[i])\n",
    "                    else:\n",
    "                        idx = np.random.randint(0, self.n_mantas)\n",
    "                        self.positions[i] = self.positions[i] + r * (self.positions[idx] - beta * self.positions[i])\n",
    "\n",
    "                # Somersault Foraging\n",
    "                r2 = np.random.random()\n",
    "                self.positions[i] = self.positions[i] + 0.5 * (self.best_position + self.positions[i]) * (2 * r2 - 1)\n",
    "\n",
    "                # Garantir que as posições estejam dentro dos limites\n",
    "                self.positions[i] = np.clip(self.positions[i], self.bounds[0], self.bounds[1])\n",
    "\n",
    "        return self.best_position, self.best_fitness\n",
    "\n",
    "# MRFO para otimizar hiperparâmetros\n",
    "n_repetitions = 5\n",
    "bounds = [\n",
    "    [-3.0458, -2.9586],  # log10(learning_rate): [0.0009, 0.0011]\n",
    "]\n",
    "\n",
    "mrfo = MRFO(lambda params, reps: evaluate_manta(params, reps), bounds, n_mantas=30, max_iter=10, patience=7, n_repetitions=n_repetitions)\n",
    "best_position, best_fitness = mrfo.optimize()\n",
    "\n",
    "# Mapear a melhor posição para hiperparâmetros\n",
    "best_lr = 10 ** best_position[0]\n",
    "\n",
    "# Treinar o modelo com a melhor configuração para obter métricas finais\n",
    "with mlflow.start_run(run_name=\"Best_MRFO_Run\"):\n",
    "    # Registrar hiperparâmetros\n",
    "    mlflow.log_param(\"learning_rate\", best_lr)\n",
    "    mlflow.log_param(\"num_layers\", 2)\n",
    "    mlflow.log_param(\"nhead\", 4)\n",
    "    mlflow.log_param(\"dim_feedforward\", 512)\n",
    "    mlflow.log_param(\"seq_length\", SEQ_LENGTH)\n",
    "    mlflow.log_param(\"resample_interval\", \"1min\")\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
    "\n",
    "    print(f\"\\nBest MRFO Configuration: LR={best_lr}, Layers=2, Heads=4, FF=512\")\n",
    "    \n",
    "    # Executar repetições sequencialmente (GPU não se beneficia de paralelismo aqui)\n",
    "    results = [train_and_evaluate(best_lr) for _ in range(n_repetitions)]\n",
    "\n",
    "    mae_list = [result[0] for result in results]\n",
    "    rmse_list = [result[1] for result in results]\n",
    "    mape_list = [result[2] for result in results]\n",
    "    smape_list = [result[3] for result in results]\n",
    "    models = [result[4] for result in results]\n",
    "\n",
    "    for rep, (mae, rmse, mape, smape_val, _) in enumerate(results):\n",
    "        print(f\"  Repetition {rep+1}/{n_repetitions}\")\n",
    "        print(f\"    MAE: {mae}, RMSE: {rmse}, MAPE: {mape}%, SMAPE: {smape_val}%\")\n",
    "\n",
    "    avg_mae = np.mean(mae_list)\n",
    "    avg_rmse = np.mean(rmse_list)\n",
    "    avg_mape = np.mean(mape_list)\n",
    "    avg_smape = np.mean(smape_list)\n",
    "    std_mae = np.std(mae_list)\n",
    "    std_rmse = np.std(rmse_list)\n",
    "    std_mape = np.std(mape_list)\n",
    "    std_smape = np.std(smape_list)\n",
    "\n",
    "    print(f\"  Average MAE: {avg_mae} (±{std_mae}), Average RMSE: {avg_rmse} (±{std_rmse})\")\n",
    "    print(f\"  Average MAPE: {avg_mape}% (±{std_mape}), Average SMAPE: {avg_smape}% (±{std_smape})\")\n",
    "\n",
    "    # Registrar métricas no MLflow\n",
    "    mlflow.log_metric(\"avg_mae\", avg_mae)\n",
    "    mlflow.log_metric(\"std_mae\", std_mae)\n",
    "    mlflow.log_metric(\"avg_rmse\", avg_rmse)\n",
    "    mlflow.log_metric(\"std_rmse\", std_rmse)\n",
    "    mlflow.log_metric(\"avg_mape\", avg_mape)\n",
    "    mlflow.log_metric(\"std_mape\", std_mape)\n",
    "    mlflow.log_metric(\"avg_smape\", avg_smape)\n",
    "    mlflow.log_metric(\"std_smape\", std_smape)\n",
    "\n",
    "    best_model = models[0]\n",
    "    mlflow.pytorch.log_model(best_model, \"best_model\")\n",
    "\n",
    "# 9. Fazer previsões\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_train_pred = best_model(X_train)\n",
    "    y_test_pred = best_model(X_test)\n",
    "\n",
    "# 10. Reverter o escalonamento e converter para MB\n",
    "y_train_pred_mb = scaler.inverse_transform(y_train_pred.cpu().numpy()) / MB\n",
    "y_train_mb = scaler.inverse_transform(y_train.cpu().numpy()) / MB\n",
    "y_test_pred_mb = scaler.inverse_transform(y_test_pred.cpu().numpy()) / MB\n",
    "y_test_mb = scaler.inverse_transform(y_test.cpu().numpy()) / MB\n",
    "\n",
    "# 11. Preparar dados para plotagem\n",
    "train_df = pd.DataFrame({\n",
    "    'date': y_dates_train,\n",
    "    'actual': y_train_mb.flatten(),\n",
    "    'predicted': y_train_pred_mb.flatten()\n",
    "}).sort_values('date')\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'date': y_dates_test,\n",
    "    'actual': y_test_mb.flatten(),\n",
    "    'predicted': y_test_pred_mb.flatten()\n",
    "}).sort_values('date')\n",
    "\n",
    "# 12. Plotar os resultados\n",
    "plt.style.use('default')\n",
    "fig, axs = plt.subplots(2, 1, figsize=(15, 10), sharex=False)\n",
    "\n",
    "axs[0].plot(train_df['date'], train_df['actual'], label='Real', color='blue', linewidth=1.5)\n",
    "axs[0].plot(train_df['date'], train_df['predicted'], label='Predito', color='red', alpha=0.7, linewidth=1.5)\n",
    "axs[0].set_title('Conjunto de Treinamento (60%)', fontsize=12, pad=10)\n",
    "axs[0].set_ylabel('Consumo de Memória (MB)', fontsize=10)\n",
    "axs[0].legend(loc='upper left', fontsize=10)\n",
    "axs[0].grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "axs[1].plot(test_df['date'], test_df['actual'], label='Real', color='blue', linewidth=1.5)\n",
    "axs[1].plot(test_df['date'], test_df['predicted'], label='Predito', color='red', alpha=0.7, linewidth=1.5)\n",
    "axs[1].set_title('Conjunto de Teste (20%)', fontsize=12, pad=10)\n",
    "axs[1].set_xlabel('Data', fontsize=10)\n",
    "axs[1].set_ylabel('Consumo de Memória (MB)', fontsize=10)\n",
    "axs[1].legend(loc='upper left', fontsize=10)\n",
    "axs[1].grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "for ax in axs:\n",
    "    ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    ax.tick_params(axis='x', rotation=45, labelsize=9)\n",
    "    ax.tick_params(axis='y', labelsize=9)\n",
    "\n",
    "plt.suptitle('Predições do Transformer Otimizado - Prometheus (MB, Resample 1min)', fontsize=14, y=0.98)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.savefig(os.path.join(DATA_DIR, 'prometheus_transformer_mrfo_1min.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
